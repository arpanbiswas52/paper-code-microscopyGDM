{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpanbiswas52/paper-code-microscopyGDM/blob/main/src/GDM_AFM(Notebook).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A Fourier-Based Global Denoising Model for Smart Artifacts Removing of Microscopy Images**\n",
        "Oct, 2025\n",
        "\n",
        "- Model developed by **Huanhuan Zhao** and **Arpan Biswas**\n",
        "- Analysis by **Huanhuan Zhao** and **Arpan Biswas**\n",
        "- Pantoea sp. YR343 biofilm data prepared by **Ruben Millan-Solsona** and **Spenser R. Brown**\n",
        "- Project conceived and supervised by **Arpan Biswas** and **Wonhee Ko**\n"
      ],
      "metadata": {
        "id": "sBl1yoa4LHin"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzhlnap0GMaS"
      },
      "source": [
        "The notebook provides the workflow of our Global Denoising Model where we need 1-2 non-pair training images, generated from simulation or experiments or both.\n",
        "\n",
        "- The integral architecture of the model builds on a lighweight Unet model\n",
        "\n",
        "- The training images are prepared based on goal-specific pre-processing of raw simulated and experimental data, as per availability. The goal here is to dissolve the known artifacts and/or enhance the known features.\n",
        "\n",
        "- The architecture has the flexibility to tune the weighting parameter which defines the percentage of information used from each image channel during training process\n",
        "\n",
        "- Here, a FFT based loss function is also integrated with tradtional pixel based MSE loss function. Minimizing the FFT loss help to minimize the random noise and maximize the preservation of the key features.\n",
        "\n",
        "- Here, we demonsrated the application on AFM-generated Pantoea sp. YR343 bio-film images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG9iDjXbG4zq",
        "outputId": "b38a50d9-4d48-4b20-ad9c-981b676e14f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting brisque\n",
            "  Downloading brisque-0.0.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from brisque) (2.0.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from brisque) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from brisque) (1.16.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from brisque) (4.12.0.88)\n",
            "Collecting libsvm-official (from brisque)\n",
            "  Downloading libsvm-official-3.36.0.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from brisque) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->brisque) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->brisque) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->brisque) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->brisque) (2025.10.5)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->brisque) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->brisque) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->brisque) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->brisque) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->brisque) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->brisque) (0.4)\n",
            "Downloading brisque-0.0.17-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: libsvm-official\n",
            "  Building wheel for libsvm-official (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm-official: filename=libsvm_official-3.36.0-cp312-cp312-linux_x86_64.whl size=124635 sha256=16df04290359c529b18fda6c520442bf624e9e8c90e58b2dcdca52f570deb283\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/65/4b/c3cdece6e5fa7eebef116be2d5a309f7ac50c90183cbe12c92\n",
            "Successfully built libsvm-official\n",
            "Installing collected packages: libsvm-official, brisque\n",
            "Successfully installed brisque-0.0.17 libsvm-official-3.36.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pillow tqdm matplotlib\n",
        "!pip install brisque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hbaUQZZaG67T"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from brisque import BRISQUE\n",
        "\n",
        "\n",
        "from skimage import io, img_as_float\n",
        "#from skimage.metrics import niqe\n",
        "import glob\n",
        "\n",
        "from math import sqrt\n",
        "from skimage.feature import blob_dog, blob_log, blob_doh\n",
        "from skimage.color import rgb2gray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A9ouf0oHFFB",
        "outputId": "d56c1a6d-3282-47d3-dab7-7920602d5691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCqYUDxWWt4g"
      },
      "source": [
        "# Some helper function to measure image quality without reference/ground truth image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F_ZwG5ZxXTdV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.feature import peak_local_max\n",
        "\n",
        "def stm_fft_quality(image_input, peak_radius=10, bg_radius=30, patch_size=1, plot= False):\n",
        "    # Load image in grayscale\n",
        "    # Convert to grayscale NumPy array\n",
        "    peak_radius = peak_radius/patch_size\n",
        "    bg_radius = bg_radius/patch_size\n",
        "\n",
        "    img_np = np.array(image_input)\n",
        "    if img_np.ndim == 3:\n",
        "        img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        img = img_np\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    img /= img.max()\n",
        "\n",
        "    # Compute FFT\n",
        "    fft_img = np.fft.fftshift(np.fft.fft2(img))\n",
        "    magnitude_spectrum = np.abs(fft_img)\n",
        "\n",
        "    # Compute center (DC component)\n",
        "    cy, cx = magnitude_spectrum.shape[0]//2, magnitude_spectrum.shape[1]//2\n",
        "\n",
        "    # Create mask for peaks and background\n",
        "    y, x = np.ogrid[:magnitude_spectrum.shape[0], :magnitude_spectrum.shape[1]]\n",
        "    r = np.sqrt((x-cx)**2 + (y-cy)**2)\n",
        "\n",
        "    # Signal region: frequencies between peak_radius and bg_radius\n",
        "    signal_mask = (r >= peak_radius) & (r <= bg_radius)\n",
        "    background_mask = (r > bg_radius)\n",
        "\n",
        "    # Compute mean intensity in signal vs background\n",
        "    signal_power = magnitude_spectrum[signal_mask].mean()\n",
        "    noise_power = magnitude_spectrum[background_mask].mean()\n",
        "\n",
        "    # FFT quality score = Signal-to-Noise Ratio (SNR) in dB\n",
        "    snr_db = 10 * np.log10(signal_power / noise_power)\n",
        "\n",
        "    if plot == True:\n",
        "      # Visualization\n",
        "      plt.figure(figsize=(10,4))\n",
        "      plt.subplot(1,2,1)\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      plt.title('STM Image')\n",
        "      plt.axis('off')\n",
        "\n",
        "      plt.subplot(1,2,2)\n",
        "      plt.imshow(np.log1p(magnitude_spectrum), cmap='inferno')\n",
        "      plt.title('FFT Magnitude Spectrum')\n",
        "      plt.axis('off')\n",
        "      plt.show()\n",
        "\n",
        "    return snr_db\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "# Upgraded version\n",
        "def stm_fft_peak_quality(image_input, min_distance=10, peak_prominence=0.05, patch_size=1, win= True, plot= False):\n",
        "    # Load STM image in grayscale\n",
        "    # Convert to grayscale NumPy array\n",
        "    peak_prominence = peak_prominence/patch_size\n",
        "    min_distance = min_distance/patch_size\n",
        "\n",
        "    img_np = np.array(image_input)\n",
        "    if img_np.ndim == 3:\n",
        "        img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        img = img_np\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    img /= img.max()\n",
        "\n",
        "    # Apply window to reduce edge artifacts\n",
        "    if win == True:\n",
        "      window = np.outer(np.hanning(img.shape[0]), np.hanning(img.shape[1]))\n",
        "      img_win = img * window\n",
        "    else:\n",
        "      img_win = img\n",
        "\n",
        "    # FFT and magnitude spectrum\n",
        "    fft_img = np.fft.fftshift(np.fft.fft2(img_win))\n",
        "    magnitude_spectrum = np.abs(fft_img)\n",
        "    log_spectrum = np.log1p(magnitude_spectrum)\n",
        "\n",
        "    # Detect peaks (exclude center DC peak)\n",
        "    cy, cx = img.shape[0] // 2, img.shape[1] // 2\n",
        "    coordinates = peak_local_max(\n",
        "        log_spectrum,\n",
        "        min_distance=int(min_distance),\n",
        "        threshold_abs=int(peak_prominence * log_spectrum.max())\n",
        "    )\n",
        "\n",
        "    # Remove central DC component\n",
        "    coordinates = [c for c in coordinates if np.hypot(c[0] - cy, c[1] - cx) > min_distance]\n",
        "\n",
        "    # Compute peak power and noise floor\n",
        "    peak_values = [magnitude_spectrum[y, x] for y, x in coordinates]\n",
        "    if len(peak_values) == 0:\n",
        "        print(\"No lattice peaks detected — image likely low quality.\")\n",
        "        return None\n",
        "\n",
        "    peak_power = np.mean(peak_values)\n",
        "\n",
        "    # Noise region: outside 50% of the max radius\n",
        "    y, x = np.ogrid[:img.shape[0], :img.shape[1]]\n",
        "    r = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
        "    noise_mask = r > 0.5 * r.max()\n",
        "    noise_power = magnitude_spectrum[noise_mask].mean()\n",
        "\n",
        "    # Peak-to-Noise Ratio (PNR) in dB\n",
        "    pnr_db = 10 * np.log10(peak_power / noise_power)\n",
        "\n",
        "    if plot == True:\n",
        "      #Plot results\n",
        "      plt.figure(figsize=(12,5))\n",
        "      plt.subplot(1,2,1)\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      plt.title(\"STM Image\")\n",
        "      plt.axis(\"off\")\n",
        "\n",
        "      plt.subplot(1,2,2)\n",
        "      plt.imshow(log_spectrum, cmap='inferno')\n",
        "      #plt.scatter([c[1] for c in coordinates], [c[0] for c in coordinates],\n",
        "      #            marker='o', facecolors='none', edgecolors='cyan', s=80, label='Detected Peaks')\n",
        "      plt.title(\"FFT Magnitude Spectrum + Peaks\")\n",
        "      plt.axis(\"off\")\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "    return pnr_db\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T77MpigTOK4P"
      },
      "outputs": [],
      "source": [
        "# This function measure the quality of the images via FFT peak intensity plots and the calculated Peak-to-Noise (PNR) scores (higher the better)\n",
        "def showFFTScore(img,den):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "    axes[0].imshow(img.convert(\"L\"), cmap=\"gray\")\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(den, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Denoised\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Test Image\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Check quality -BRISQUE # lower = better\n",
        "    #obj = BRISQUE(url=False)\n",
        "    #print(\"Original image (BRISQUE):\" +str(obj.score(img)))\n",
        "    #print(\"Denoise image (BRISQUE):\" +str(obj.score(den.convert(\"RGB\"))))\n",
        "\n",
        "    score1 = stm_fft_quality(np.array(img), plot=True)\n",
        "    print(f\"Basic FFT-based quality score (SNR in dB): Original image: {score1:.2f} dB\") #Higher is better\n",
        "    score1 = stm_fft_quality(np.array(den), plot=True)\n",
        "    print(f\"Basic FFT-based quality score (SNR in dB): Denoise image: {score1:.2f} dB\") #Higher is better\n",
        "\n",
        "    score2 = stm_fft_peak_quality(np.array(img), plot=True)\n",
        "    if score2 is not None:\n",
        "      print(f\"Upgraded FFT-based quality score (SNR in dB): Original image: {score2:.2f} dB\") #Higher is better\n",
        "    score2 = stm_fft_peak_quality(np.array(den), plot=True)\n",
        "    if score2 is not None:\n",
        "      print(f\"Upgraded FFT-based quality score (SNR in dB): Denoise image: {score2:.2f} dB\") #Higher is better\n",
        "\n",
        "\n",
        "##################################################################################################################\n",
        "# This function measure the quality of the images via detected lines plots\n",
        "# and the calculated total line length within the range of angle, representing the artifacts (lower the better).\n",
        "from skimage import feature\n",
        "from skimage.transform import probabilistic_hough_line\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_detected_lines_angle_range(img,\n",
        "                                    sigma=0.1,\n",
        "                                    low_threshold=1,\n",
        "                                    high_threshold=10,\n",
        "                                    line_length=1,\n",
        "                                    line_gap=2,\n",
        "                                    angle_min=-50,\n",
        "                                    angle_max=-35):\n",
        "    \"\"\"\n",
        "    Detect fine lines in STM-like images, draw only lines in the specified angle range,\n",
        "    return the total length of lines within that range, and show histogram of all line angles.\n",
        "    \"\"\"\n",
        "\n",
        "    # if not isinstance(img, np.ndarray):\n",
        "    #     img = img.detach().cpu().numpy()\n",
        "    img = np.array(img)\n",
        "    img = img.astype(np.float32)\n",
        "    if img.max() > 1:\n",
        "        img = img / 255.0\n",
        "\n",
        "    # Edge detection\n",
        "    edges = feature.canny(img, sigma=sigma,\n",
        "                          low_threshold=low_threshold/255.0,\n",
        "                          high_threshold=high_threshold/255.0)\n",
        "\n",
        "    # Hough line transform\n",
        "    lines = probabilistic_hough_line(edges,\n",
        "                                     threshold=5,\n",
        "                                     line_length=line_length,\n",
        "                                     line_gap=line_gap)\n",
        "\n",
        "    total_length_in_range = 0.0\n",
        "    angles = []\n",
        "    lines_in_range = []\n",
        "    lengths = []\n",
        "\n",
        "    # Filter lines by angle\n",
        "    for (p0, p1) in lines:\n",
        "        dx = p1[0] - p0[0]\n",
        "        dy = p1[1] - p0[1]\n",
        "        angle = np.degrees(np.arctan2(dy, dx))\n",
        "        length = np.sqrt(dx**2 + dy**2)\n",
        "        angles.append(angle)\n",
        "        lengths.append(length)\n",
        "        if angle_min <= angle <= angle_max:\n",
        "            total_length_in_range += length\n",
        "            lines_in_range.append((p0, p1))\n",
        "\n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 18))\n",
        "    ax = axes.ravel()\n",
        "\n",
        "    ax[0].imshow(img, cmap=\"gray\")\n",
        "    ax[0].set_title(\"Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "\n",
        "    ax[1].imshow(edges, cmap=\"gray\")\n",
        "    for (p0, p1) in lines_in_range:\n",
        "        ax[1].plot((p0[0], p1[0]), (p0[1], p1[1]), 'r-', linewidth=1)\n",
        "    ax[1].set_title(f\"Lines in [{angle_min}, {angle_max}]°\\nTotal length = {total_length_in_range:.2f} px\")\n",
        "    ax[1].axis(\"off\")\n",
        "\n",
        "    ax[2].hist(angles, bins=36, range=(-90, 90), color=\"blue\", alpha=0.7)\n",
        "    ax[2].set_title(\"Histogram of Line Angles\",fontsize=20)\n",
        "    ax[2].set_xlabel(\"Angle (degrees)\",fontsize=20)\n",
        "    ax[2].set_ylabel(\"Count\",fontsize=20)\n",
        "\n",
        "    ax[3].hist(lengths, bins=np.arange(0, 101, 5), color=\"green\", alpha=0.7)\n",
        "\n",
        "    ax[3].set_title(\"Histogram of Line Lengths\",fontsize=20)\n",
        "    ax[3].set_xlabel(\"Line Length (pixels)\",fontsize=20)\n",
        "    ax[3].set_ylabel(\"Count\",fontsize=20)\n",
        "    plt.xticks(fontsize=20, fontfamily='monospace', fontstyle='italic')\n",
        "    plt.yticks(fontsize=20, fontfamily='monospace', fontstyle='italic')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return total_length_in_range\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FFT based loss functions\n",
        "\n",
        "- In this paper, we have used the 2nd function"
      ],
      "metadata": {
        "id": "RF2DLteJDP8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cal fft loss for training denoise model\n",
        "def fft_loss(pred, target):\n",
        "    # Apply FFT\n",
        "    pred_fft = torch.fft.fft2(pred)\n",
        "    target_fft = torch.fft.fft2(target)\n",
        "\n",
        "    # Shift zero frequency to center\n",
        "    pred_fft = torch.fft.fftshift(pred_fft)\n",
        "    target_fft = torch.fft.fftshift(target_fft)\n",
        "\n",
        "    # Use magnitude (ignore phase)\n",
        "    pred_mag = torch.abs(pred_fft)\n",
        "    target_mag = torch.abs(target_fft)\n",
        "\n",
        "    # Normalize for stability\n",
        "    pred_mag = pred_mag / (pred_mag.mean() + 1e-8)\n",
        "    target_mag = target_mag / (target_mag.mean() + 1e-8)\n",
        "\n",
        "    # L1 loss in frequency domain\n",
        "    return torch.mean(torch.abs(pred_mag - target_mag))\n",
        "\n",
        "# FFT-based quality score (higher is better, normalized cross-correlation in frequency domain)\n",
        "def fft_quality(pred, target):\n",
        "    \"\"\"\n",
        "    FFT-based cosine similarity between predicted and target images.\n",
        "    Returns a score in [0, 1] (higher = better).\n",
        "    \"\"\"\n",
        "    # Compute FFT\n",
        "    pred_fft = torch.fft.fft2(pred)\n",
        "    target_fft = torch.fft.fft2(target)\n",
        "\n",
        "    # Shift zero-frequency to center (for interpretability)\n",
        "    pred_fft = torch.fft.fftshift(pred_fft)\n",
        "    target_fft = torch.fft.fftshift(target_fft)\n",
        "\n",
        "    # Magnitude spectra\n",
        "    pred_mag = torch.abs(pred_fft).flatten()\n",
        "    target_mag = torch.abs(target_fft).flatten()\n",
        "\n",
        "    # Cosine similarity in frequency domain\n",
        "    score = torch.dot(pred_mag, target_mag) / (\n",
        "        pred_mag.norm() * target_mag.norm() + 1e-8\n",
        "    )\n",
        "\n",
        "    return score.item()"
      ],
      "metadata": {
        "id": "MWh0Ld5JDSO7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising model architecture and training module"
      ],
      "metadata": {
        "id": "LKR34zDxDfEC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PIVYadTfGJYd"
      },
      "outputs": [],
      "source": [
        "# -------------------- Simple U-Net model --------------------\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UNetSmall(nn.Module):\n",
        "    def __init__(self, in_ch=1, base=32):\n",
        "        super().__init__()\n",
        "        self.enc1 = DoubleConv(in_ch, base)\n",
        "        self.enc2 = DoubleConv(base, base*2)\n",
        "        self.enc3 = DoubleConv(base*2, base*4)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.dec3 = DoubleConv(base*6, base*2)\n",
        "        self.dec2 = DoubleConv(base*3, base)\n",
        "        self.final = nn.Conv2d(base, in_ch, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        d3 = self.up(e3)\n",
        "        d3 = torch.cat([d3, e2], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "        d2 = self.up(d3)\n",
        "        d2 = torch.cat([d2, e1], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "        out = self.final(d2)\n",
        "        return out\n",
        "\n",
        "# Function to generate image patches from single training image\n",
        "class STMNoiseSingleImageDataset(Dataset): #input is .png image\n",
        "    def __init__(self, image_path, patch_size=128):\n",
        "        if not os.path.exists(image_path):\n",
        "            raise RuntimeError(f\"File not found: {image_path}\")\n",
        "        self.img = Image.open(image_path).convert(\"L\")\n",
        "        self.patch_size = patch_size\n",
        "        self.len = 1000  # number of random patches per epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        w, h = self.img.size\n",
        "        if w < self.patch_size or h < self.patch_size:\n",
        "            self.img = self.img.resize((max(w, self.patch_size), max(h, self.patch_size)))\n",
        "            w, h = self.img.size\n",
        "        left = random.randint(0, w - self.patch_size)\n",
        "        top = random.randint(0, h - self.patch_size)\n",
        "        patch = self.img.crop((left, top, left + self.patch_size, top + self.patch_size))\n",
        "        patch = np.array(patch).astype(np.float32) / 255.0\n",
        "        patch = torch.from_numpy(patch).unsqueeze(0)\n",
        "        return patch\n",
        "\n",
        "# Function to create random mask on the image patches\n",
        "def create_random_mask(batch, mask_fraction=0.005):\n",
        "    B, C, H, W = batch.shape\n",
        "    Npix = H * W\n",
        "    nm = max(1, int(mask_fraction * Npix))\n",
        "    masks = torch.zeros_like(batch)\n",
        "    for b in range(B):\n",
        "        idxs = torch.randperm(Npix)[:nm]\n",
        "        ys = idxs // W\n",
        "        xs = idxs % W\n",
        "        masks[b, 0, ys, xs] = 1.0\n",
        "    return masks\n",
        "\n",
        "# Function to train the GDM model\n",
        "def train(w1=0.5, w2=0.5, isfft=True):\n",
        "\n",
        "    dataset1 = STMNoiseSingleImageDataset(train_image_path1, patch_size=patch_size)\n",
        "    loader1 = DataLoader(dataset1, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    dataset2 = STMNoiseSingleImageDataset(train_image_path2, patch_size=patch_size)  ### FIXED\n",
        "    loader2 = DataLoader(dataset2, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = UNetSmall(in_ch=1, base=32).to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.5)\n",
        "    mse = nn.MSELoss(reduction='none')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_fft_score = 0.0\n",
        "\n",
        "        # zip ensures parallel iteration (stops at shortest loader)\n",
        "        pbar = tqdm(zip(loader1, loader2), total=min(len(loader1), len(loader2)), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        #j = 0\n",
        "        for patches1, patches2 in pbar:\n",
        "            #j = j+1\n",
        "            # ---------------- dataset1 ----------------\n",
        "            patches1 = patches1.to(device)\n",
        "            masks1 = create_random_mask(patches1, mask_fraction=mask_fraction).to(device)\n",
        "            inp1 = patches1.clone()\n",
        "            inp1 = inp1 * (1.0 - masks1) + 0.5 * masks1\n",
        "            #print(inp1.shape)\n",
        "            out1 = model(inp1)\n",
        "            loss_map1 = mse(out1, patches1).mean(dim=1, keepdim=True)\n",
        "            pixel_loss1 = (loss_map1 * masks1).sum() / (masks1.sum() + 1e-8)\n",
        "            fft_score1 = fft_quality(out1, patches1)\n",
        "\n",
        "            # ---------------- dataset2 ----------------\n",
        "            patches2 = patches2.to(device)\n",
        "            masks2 = create_random_mask(patches2, mask_fraction=mask_fraction).to(device)\n",
        "            inp2 = patches2.clone()\n",
        "            inp2 = inp2 * (1.0 - masks2) + 0.5 * masks2\n",
        "            out2 = model(inp2)\n",
        "            #print(inp1.shape)\n",
        "            loss_map2 = mse(out2, patches2).mean(dim=1, keepdim=True)\n",
        "            pixel_loss2 = (loss_map2 * masks2).sum() / (masks2.sum() + 1e-8)\n",
        "            fft_score2 = fft_quality(out2, patches2)\n",
        "\n",
        "            # ---------------- combine ----------------\n",
        "            w_pixel_loss = w1 * pixel_loss1 + w2 * pixel_loss2\n",
        "            w_fft_score = w1 * fft_score1 + w2 * fft_score2\n",
        "\n",
        "            if isfft == True:\n",
        "              loss = w_pixel_loss / (1 + w_fft_score)\n",
        "            else:\n",
        "              loss = w_pixel_loss\n",
        "              w_fft_score = 0\n",
        "            #loss = w_pixel_loss / (1 + fft_score2)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_fft_score += w_fft_score\n",
        "            #running_fft_score += fft_score2\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{running_loss:.4f}\",\n",
        "                \"fft_score\": f\"{running_fft_score / (pbar.n+1):.4f}\"\n",
        "            })\n",
        "        #print(j)\n",
        "        scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to test the GDM model- We call this once we train the GDM and want to denoise a test noisy experimental image. The code returns the denoised image\n",
        "def denoise_image(model, pil_img, patch_size=128, stride=64):\n",
        "    model.eval()\n",
        "    img = pil_img.convert(\"L\")\n",
        "    arr = np.array(img).astype(np.float32) / 255.0\n",
        "    H, W = arr.shape\n",
        "    pad_h = (patch_size - (H % stride)) % stride\n",
        "    pad_w = (patch_size - (W % stride)) % stride\n",
        "    padded = np.pad(arr, ((0, pad_h), (0, pad_w)), mode='reflect')\n",
        "    Ph, Pw = padded.shape\n",
        "    out_img = np.zeros_like(padded)\n",
        "    weight = np.zeros_like(padded)\n",
        "    with torch.no_grad():\n",
        "        for top in range(0, Ph - patch_size + 1, stride):\n",
        "            for left in range(0, Pw - patch_size + 1, stride):\n",
        "                patch = padded[top:top+patch_size, left:left+patch_size]\n",
        "                x = torch.from_numpy(patch).unsqueeze(0).unsqueeze(0).to(device)\n",
        "                pred = model(x).cpu().numpy()[0,0]\n",
        "                out_img[top:top+patch_size, left:left+patch_size] += pred\n",
        "                weight[top:top+patch_size, left:left+patch_size] += 1.0\n",
        "    out_img = out_img / (weight + 1e-8)\n",
        "    out_img = out_img[:H, :W]\n",
        "    out_img = (out_img * 255.0).clip(0,255).astype(np.uint8)\n",
        "    return Image.fromarray(out_img)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study 2\n",
        "- Pantoea sp. YR343 biofilm images"
      ],
      "metadata": {
        "id": "BEYOP06wt7w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define source folder path (inside your Google Drive)\n",
        "#    Example: if folder is in \"MyDrive/my_project_data\"\n",
        "source_folder = '/content/drive/My Drive/Colab Notebooks/BRAVE Project/AFM/Data/PNG'\n",
        "\n",
        "# 3. Define destination folder path (inside Colab's working directory)\n",
        "destination_folder = '/content/AFM_data'\n",
        "\n",
        "# 4. Copy the folder from Drive to Colab\n",
        "if os.path.exists(source_folder):\n",
        "    shutil.copytree(source_folder, destination_folder)\n",
        "    print(f\"Folder copied to {destination_folder}\")\n",
        "else:\n",
        "    print(f\"Source folder not found: {source_folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVgdhnHrt13b",
        "outputId": "fdcf3be2-a673-4769-cce1-a3b524769e44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder copied to /content/AFM_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal-Specific Image Preprocessing\n",
        "- Disolves the horizontal scan lines (artifacts)\n",
        "- Preparing the image for channel 1"
      ],
      "metadata": {
        "id": "vfi4wanCJ-ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks, detrend\n",
        "from scipy.ndimage import gaussian_filter1d, binary_dilation\n",
        "\n",
        "def remove_horizontal_lines_darkregions(\n",
        "    img_path,\n",
        "    peak_smooth_sigma=3,\n",
        "    peak_prominence=0.2,\n",
        "    notch_halfwidth=3,\n",
        "    dark_percentile=45,\n",
        "    exclude_center_radius=8,\n",
        "    show=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Remove horizontal scan-lines, with special handling to remove them completely\n",
        "    only in dark regions of the image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_path : str\n",
        "        Path to grayscale AFM/STM image.\n",
        "    peak_smooth_sigma : float\n",
        "        Smoothing sigma applied to column-energy before peak detection.\n",
        "    peak_prominence : float in (0,1)\n",
        "        Peak detection prominence threshold relative to max column energy.\n",
        "    notch_halfwidth : int\n",
        "        Half width (in pixels) of the notch around detected vertical-frequency columns.\n",
        "    dark_percentile : float (0-100)\n",
        "        Percentile used to define 'dark' pixels in the image (lower values = fewer dark pixels).\n",
        "    exclude_center_radius : int\n",
        "        Number of pixels around the FFT center to exclude from notching (avoids killing DC).\n",
        "    show : bool\n",
        "        If True, display diagnostic plots.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cleaned_uint8 : np.ndarray\n",
        "        Cleaned AFM/STM image (uint8).\n",
        "    mask_img : np.ndarray\n",
        "        FFT mask image (uint8, white=kept, black=masked).\n",
        "    dark_mask_img : np.ndarray\n",
        "        Dark-region mask (uint8, white=detrending applied, black=untouched).\n",
        "    diagnostics : dict\n",
        "        Useful internals for debugging.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Load image (grayscale) ---\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "    H, W = img.shape\n",
        "\n",
        "    # Normalize to zero-mean\n",
        "    img_mean, img_std = img.mean(), img.std() + 1e-12\n",
        "    img_norm = (img - img_mean) / img_std\n",
        "\n",
        "    # --- FFT ---\n",
        "    F = np.fft.fftshift(np.fft.fft2(img_norm))\n",
        "    mag = np.abs(F)\n",
        "    logmag = np.log1p(mag)\n",
        "\n",
        "    # Column-wise energy\n",
        "    col_energy = mag.sum(axis=0)\n",
        "    col_energy_s = gaussian_filter1d(col_energy, sigma=peak_smooth_sigma)\n",
        "\n",
        "    # Exclude DC\n",
        "    cx = W // 2\n",
        "    exclude_mask_cols = np.zeros_like(col_energy_s, dtype=bool)\n",
        "    exclude_mask_cols[max(0, cx-exclude_center_radius):min(W, cx+exclude_center_radius+1)] = True\n",
        "    col_energy_for_peaks = col_energy_s.copy()\n",
        "    col_energy_for_peaks[exclude_mask_cols] = 0.0\n",
        "\n",
        "    # Detect streak columns\n",
        "    height_thresh = peak_prominence * col_energy_for_peaks.max()\n",
        "    peaks, _ = find_peaks(col_energy_for_peaks, height=height_thresh)\n",
        "    peaks_all = list(peaks)\n",
        "    for p in peaks:\n",
        "        p_sym = 2*cx - p\n",
        "        if 0 <= p_sym < W:\n",
        "            peaks_all.append(int(p_sym))\n",
        "    peaks_all = sorted(set(peaks_all))\n",
        "\n",
        "    # --- Build FFT mask ---\n",
        "    mask = np.ones_like(F, dtype=bool)\n",
        "    for p in peaks_all:\n",
        "        lo, hi = max(0, p - notch_halfwidth), min(W, p + notch_halfwidth + 1)\n",
        "        mask[:, lo:hi] = False\n",
        "\n",
        "    # Keep DC\n",
        "    cy = H // 2\n",
        "    Y, X = np.ogrid[:H, :W]\n",
        "    R = np.sqrt((X-cx)**2 + (Y-cy)**2)\n",
        "    mask[R < exclude_center_radius] = True\n",
        "\n",
        "    # Apply mask\n",
        "    F_notched = F * mask\n",
        "    img_ifft_notched = np.fft.ifft2(np.fft.ifftshift(F_notched)).real\n",
        "    img_ifft_rescaled = (img_ifft_notched * img_std) + img_mean\n",
        "\n",
        "    # --- Dark-region mask ---\n",
        "    dark_threshold = np.percentile(img_ifft_rescaled, dark_percentile)\n",
        "    dark_mask = img_ifft_rescaled <= dark_threshold\n",
        "    dark_mask = binary_dilation(dark_mask, structure=np.ones((3,7)))\n",
        "\n",
        "    # --- Row-wise detrend in dark regions ---\n",
        "    img_detrend = np.zeros_like(img_ifft_rescaled)\n",
        "    for r_idx in range(H):\n",
        "        row = img_ifft_rescaled[r_idx, :]\n",
        "        row_detr = detrend(row, type='linear')\n",
        "        new_row = row.copy()\n",
        "        new_row[dark_mask[r_idx, :]] = row_detr[dark_mask[r_idx, :]]\n",
        "        img_detrend[r_idx, :] = new_row\n",
        "\n",
        "    # Normalize final outputs\n",
        "    cleaned_uint8 = cv2.normalize(img_detrend, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    mask_img = (mask.astype(np.uint8)) * 255\n",
        "    dark_mask_img = (dark_mask.astype(np.uint8)) * 255\n",
        "\n",
        "    diagnostics = {\n",
        "        \"img_norm\": img_norm,\n",
        "        \"logmag\": logmag,\n",
        "        \"col_energy\": col_energy,\n",
        "        \"col_energy_s\": col_energy_s,\n",
        "        \"detected_peaks\": peaks_all,\n",
        "        \"fft_mask\": mask,\n",
        "        \"dark_mask\": dark_mask\n",
        "    }\n",
        "\n",
        "    if show:\n",
        "        plt.figure(figsize=(18,6))\n",
        "        plt.subplot(1,4,1); plt.imshow(img, cmap='gray'); plt.title(\"Original\"); plt.axis(\"off\")\n",
        "        plt.subplot(1,4,2); plt.imshow(cleaned_uint8, cmap='gray'); plt.title(\"Cleaned\"); plt.axis(\"off\")\n",
        "        plt.subplot(1,4,3); plt.imshow(mask_img, cmap='gray'); plt.title(\"FFT Mask\"); plt.axis(\"off\")\n",
        "        plt.subplot(1,4,4); plt.imshow(dark_mask_img, cmap='gray'); plt.title(\"Dark Mask\"); plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return cleaned_uint8, mask_img, dark_mask_img, diagnostics\n"
      ],
      "metadata": {
        "id": "My6NQXpIug_g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_path = \"/content/AFM_data/afm_image1.png\"\n",
        "#Example usage (adjust path and parameters to taste):\n",
        "cleaned, masked, dark_mask, diag = remove_horizontal_lines_darkregions(train_image_path,\n",
        "                                                    peak_smooth_sigma=0.1,\n",
        "                                                    peak_prominence=0.2,\n",
        "                                                    notch_halfwidth=1,\n",
        "                                                    dark_percentile=50,\n",
        "                                                    exclude_center_radius=50,\n",
        "                                                    show=True)"
      ],
      "metadata": {
        "id": "iKYDmFItulwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_image_pil = Image.fromarray(cleaned)\n",
        "dark_mask_pil = Image.fromarray(dark_mask)\n",
        "\n",
        "# Save as .png files\n",
        "cleaned_image_path = \"cleaned_image.png\"\n",
        "dark_mask_path = \"dark_mask.png\"\n",
        "\n",
        "cleaned_image_pil.save(cleaned_image_path)\n",
        "dark_mask_pil.save(dark_mask_path)"
      ],
      "metadata": {
        "id": "43isGC7UJLGU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal-Specific Image Preprocessing\n",
        "- Merging the original image with dark mask to preserve the features in non-black region\n",
        "- Preparing the image for channel 2\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "h7AFQtkK7Hel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def merge_with_mask(original_path, mask_path, output_path=\"merged.png\"):\n",
        "    \"\"\"\n",
        "    Merge two images:\n",
        "      - Dark regions (black in mask) → replaced by white\n",
        "      - Non-dark regions (white in mask) → original image is kept\n",
        "    \"\"\"\n",
        "    # Load images\n",
        "    original = Image.open(original_path).convert(\"L\")  # grayscale\n",
        "    mask = Image.open(mask_path).convert(\"L\")          # grayscale mask\n",
        "\n",
        "    # Convert to numpy\n",
        "    orig_arr = np.array(original)\n",
        "    mask_arr = np.array(mask)\n",
        "\n",
        "    # Ensure same shape\n",
        "    min_h = min(orig_arr.shape[0], mask_arr.shape[0])\n",
        "    min_w = min(orig_arr.shape[1], mask_arr.shape[1])\n",
        "    orig_arr = orig_arr[:min_h, :min_w]\n",
        "    mask_arr = mask_arr[:min_h, :min_w]\n",
        "\n",
        "    # White areas in mask → keep original\n",
        "    # Black areas in mask → replace with white\n",
        "    merged_arr = np.where(mask_arr > 128, orig_arr, 255)\n",
        "\n",
        "    # Convert back to image\n",
        "    merged_img = Image.fromarray(merged_arr.astype(np.uint8))\n",
        "    merged_img.save(output_path)\n",
        "\n",
        "    return merged_img\n",
        "\n",
        "# Example usage (with your split images)\n",
        "# merged = merge_with_mask(\"original.png\", \"dark_mask.png\", \"merged.png\")\n",
        "# merged.show()\n"
      ],
      "metadata": {
        "id": "N_4if01UJXam"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def merge_shadow_with_mask(original_path, mask_path, output_path=\"merged.png\"):\n",
        "    \"\"\"\n",
        "    Replaces the shadowed (scan line) regions of the original image\n",
        "    with white where the mask has white.\n",
        "    \"\"\"\n",
        "    # Load original and mask\n",
        "    original = Image.open(original_path).convert(\"L\")  # grayscale\n",
        "    mask = Image.open(mask_path).convert(\"L\")          # grayscale mask\n",
        "\n",
        "    # Convert to numpy\n",
        "    orig_arr = np.array(original)\n",
        "    mask_arr = np.array(mask)\n",
        "\n",
        "    # Ensure same size\n",
        "    min_h = min(orig_arr.shape[0], mask_arr.shape[0])\n",
        "    min_w = min(orig_arr.shape[1], mask_arr.shape[1])\n",
        "    orig_arr = orig_arr[:min_h, :min_w]\n",
        "    mask_arr = mask_arr[:min_h, :min_w]\n",
        "\n",
        "    # White in mask → replace with white\n",
        "    # Black in mask → keep original\n",
        "    merged_arr = np.where(mask_arr > 128, 255, orig_arr)\n",
        "\n",
        "    # Save merged result\n",
        "    merged_img = Image.fromarray(merged_arr.astype(np.uint8))\n",
        "    merged_img.save(output_path)\n",
        "\n",
        "    # Optional visualization\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,3,1); plt.imshow(orig_arr, cmap=\"gray\"); plt.title(\"Original\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,2); plt.imshow(mask_arr, cmap=\"gray\"); plt.title(\"Dark Mask\"); plt.axis(\"off\")\n",
        "    plt.subplot(1,3,3); plt.imshow(merged_arr, cmap=\"gray\"); plt.title(\"Merged Result\"); plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return merged_img\n",
        "\n",
        "# Example usage:\n",
        "# merge_shadow_with_mask(\"original.png\", \"dark_mask.png\", \"merged.png\")\n"
      ],
      "metadata": {
        "id": "_T4kWyvh9JLu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final preprocessed images from single experimental image"
      ],
      "metadata": {
        "id": "Er9d8ZHMvFth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_path1 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "\n",
        "merged = merge_shadow_with_mask(train_image_path1, train_image_path2, \"merged.png\")\n",
        "merged.show()"
      ],
      "metadata": {
        "id": "boTrJ8rC7pSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=0.5, w2=0.5"
      ],
      "metadata": {
        "id": "b0zpJ261vSJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=0.5, w2=0.5)"
      ],
      "metadata": {
        "id": "sCW3Njdr-G2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model -Denoise experimental images"
      ],
      "metadata": {
        "id": "KaBIavBOvXZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=1, high_threshold=10, line_length=1, line_gap=2, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=1, high_threshold=10, line_length=1, line_gap=2, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "772guMd1-bVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=0, w2=1"
      ],
      "metadata": {
        "id": "C37HJz_s-c4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=0.00, w2=1.0)\n",
        "#"
      ],
      "metadata": {
        "id": "n14Ftnql2kOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model -Denoise all experimental images"
      ],
      "metadata": {
        "id": "be3WE811vlwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "i97Bt_oSOfty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=0.1, w2=0.9"
      ],
      "metadata": {
        "id": "JvEYZh_jvqPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=0.1, w2=0.9)"
      ],
      "metadata": {
        "id": "Vr_q3-idU-EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model -Denoise all experimental images"
      ],
      "metadata": {
        "id": "YdXVt1IsvtV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "T2QnQtv1VPy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=0.01, w2=0.99"
      ],
      "metadata": {
        "id": "O7ekJcVXvxY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=0.01, w2=0.99)"
      ],
      "metadata": {
        "id": "WgoSo_6zuZUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model- Denoise all experimental images"
      ],
      "metadata": {
        "id": "Nt0ruNQ1v0vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "I7lKCtd2usG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=0.9, w2=0.1"
      ],
      "metadata": {
        "id": "d6Hs9gGXv4QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=0.9, w2=0.1)"
      ],
      "metadata": {
        "id": "P6KHx6_-HoWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model - Denoise all experimental images"
      ],
      "metadata": {
        "id": "cA0XLs-bv8Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "CnR98eaDHzOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=1.0, w2=0.0"
      ],
      "metadata": {
        "id": "SV8H_Z47wG_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=1.0, w2=0.0)"
      ],
      "metadata": {
        "id": "sU-e3-fyiiDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model - Denoise all experimental images"
      ],
      "metadata": {
        "id": "8GM-9TBEwLYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "if4hyS9Ciqc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training model - w1=0.5, w2=0.5 without FFT loss"
      ],
      "metadata": {
        "id": "jukX-lTLwQb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Hyperparameters --------------------\n",
        "#train_image_path = \"../Desktop/Dataset1_TaS2/simulate_si.png\"  # single training image\n",
        "#train_image_path1 = \"/content/AFM_data/afm_image6.png\"\n",
        "#train_image_path2 = \"/content/AFM_data/afm_image1.png\"\n",
        "train_image_path1 = \"/content/merged.png\"\n",
        "train_image_path2 = \"/content/dark_mask.png\"\n",
        "patch_size = 128\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "mask_fraction = 0.1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# w1 = 0.9\n",
        "# w2 = 1 - w1\n",
        "# ---- Run ----\n",
        "model2 = train(w1=0.5, w2=0.5, isfft=False)"
      ],
      "metadata": {
        "id": "yIFg1KG6W-bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing model -Denoise all experimental images"
      ],
      "metadata": {
        "id": "ultia5e3wS_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path & filename parts\n",
        "#base_dir = \"../Desktop/Dataset1_TaS2/si_expe\"\n",
        "base_dir = \"/content/AFM_data/\"\n",
        "prefix = \"afm_image\"\n",
        "suffix = \".png\"\n",
        "\n",
        "# Loop through sequential file numbers\n",
        "for i in range(1, 17):  # adjust 5 to your total + 1\n",
        "    file_name = f\"{prefix}{i:d}{suffix}\"  # 1, 2, 3...\n",
        "    image_path = os.path.join(base_dir, file_name)\n",
        "    if os.path.exists(image_path):\n",
        "            img = Image.open(image_path).convert(\"RGB\")\n",
        "            width, height = img.size\n",
        "            den = denoise_image(model2, img, patch_size=width-1, stride=patch_size//2)\n",
        "            showFFTScore(img,den)\n",
        "            show_detected_lines_angle_range(img.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)\n",
        "            show_detected_lines_angle_range(den.convert(\"L\"),  sigma=1, low_threshold=5, high_threshold=10, line_length=1, line_gap=1, angle_min=-10,  angle_max=10)"
      ],
      "metadata": {
        "id": "R55a8D1jXCbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}